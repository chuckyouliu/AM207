{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
    "**Due: 11.59 P.M. Thursday April 14th, 2016**\n",
    "\n",
    "### Note: This homework is only for one week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to Canvas.\n",
    "\n",
    "+ We will provide you imports for your ipython notebook. Please do not import additional libraries.\n",
    "\n",
    "+ Your individual submissions should use the following filenames: AM207_YOURNAME_HW5.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
    "\n",
    "+ **Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. **\n",
    "\n",
    "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW5.tar.gz or AM207_YOURNAME_HW5.zip\n",
    "\n",
    "\n",
    "### Have Fun!\n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesliu/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: HMM... I Think Your Text Got Corrupted!\n",
    "\n",
    "In this problem you should use a Hidden Markov Model to correct typos in a text without using a dictionary. Your data is in two different text files:\n",
    "\n",
    "* `Shakespeare_correct.txt` contains the words of some sonnets from Shakespeare\n",
    "* `Shakespeare_typos.txt` contains the same text, but now some of the characters are corrupted\n",
    "\n",
    "For convenience both text files only contain lower case letters a-z and spaces. \n",
    "\n",
    "First build a first order HMM:\n",
    "* What are the hidden states and what are the observed states?\n",
    "* What should you do to generate your HMM probability matrices?\n",
    "* For some of the HMM parameters, you won't have enough training data to get representative probabilities.  For example, some of your probabilites might be 0. You should address this problem by adding a small pseudocount, similar to the motif finding problem from a previous assignment. \n",
    "* Implement the Viterbi algorithm and run it on a test portion that contains errors. Show that your Viterbi implementation can improve text of length 100, 500, 1000, and 2000. Note: To do this correctly you would have to withhold the part of the text that you use for testing when you estimate the parameters for you HMM. For the sake of this homework it is ok though to report training error instead of test error. Just be aware that the correction rate you are reporting most likely is a very optimistic estimate. \n",
    "* What correction rate do you get?\n",
    "\n",
    "**Important**: Wikipedia has a nice article on [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm). **Please do not use the python implementation from this article!** (The lecture notebook also has the version from Wikipedia). Using dictionaries for Viterbi is really not intuitive and using numpy is typically faster. The article has very nice pseudo code that should enable you to easily program Viterbi by yourself. Please also refrain for this problem from using any other third party implementations. \n",
    "\n",
    "Now for a second order HMM:\n",
    "By using a second order HMM, you should be able to get a better correction rate. \n",
    "* Give an intuitive explanation why a second order HMM should give better results.\n",
    "* Implement your second order text correction. Hint: If you think a bit about the model you won't even have to change your Viterbi implementation. \n",
    "* Compare your correction rates against the first order model for text length of 100 and 500, (you can do 1000 as well if your computer is fast enough). \n",
    "* How well would your implementation scale to HMMs of even higher order? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (\"Shakespeare_correct.txt\", \"r\") as txt:\n",
    "    sh_correct = list(txt.readlines()[0])\n",
    "with open (\"Shakespeare_typos.txt\", \"r\") as txt:\n",
    "    sh_typo = list(txt.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict to go from character(or space) to index number and vice versa for convenience\n",
    "charToInt = dict(zip(list(\"abcdefghijklmnopqrstuvwxyz \"), range(27)))\n",
    "intToChar = dict(zip(range(27), list(\"abcdefghijklmnopqrstuvwxyz \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map character sequences to number indices\n",
    "sh_correct_idx = map(lambda x: charToInt[x], sh_correct)\n",
    "sh_typo_idx = map(lambda x: charToInt[x], sh_typo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize starting, transition, and emission counts\n",
    "# states and emissions are just characters or space\n",
    "# initialize counts to 1 to provide pseudocount\n",
    "transition = np.ones((len(charToInt), len(charToInt)))\n",
    "emission = np.ones((len(charToInt), len(charToInt)))\n",
    "start = np.ones((len(charToInt)))\n",
    "\n",
    "# as stated in problem, don't need to separate out test set can just use entire corpus to train\n",
    "# add a count to start for every start of a new word\n",
    "# add a count to transition for every transition in correct corpus\n",
    "# add an emission count for every correct -> typo corpus\n",
    "start[sh_correct_idx[0]] += 1\n",
    "for i in range(len(sh_correct_idx)-1):\n",
    "    transition[sh_correct_idx[i],sh_correct_idx[i+1]] += 1\n",
    "    emission[sh_correct_idx[i],sh_typo_idx[i]] += 1\n",
    "    if sh_correct_idx[i] == charToInt[\" \"] and i+1 < len(sh_correct_idx):\n",
    "        start[sh_correct_idx[i+1]] += 1    \n",
    "emission[sh_correct_idx[len(sh_correct_idx)-1],sh_typo_idx[len(sh_correct_idx)-1]] += 1\n",
    "\n",
    "# move from counts to probabilities\n",
    "transition /= transition.sum(axis=1)[:, np.newaxis]\n",
    "emission /= emission.sum(axis=1)[:, np.newaxis]\n",
    "start /= np.sum(start)\n",
    "\n",
    "# move to log probabilities\n",
    "transition = np.log(transition)\n",
    "emission = np.log(emission)\n",
    "start = np.log(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# viterbi implementation\n",
    "# obs is the text with letters mapped to charToInt\n",
    "# hmm params from cell above\n",
    "def viterbi(obs, start_p, trans_p, emit_p):\n",
    "    # create V matrix of probabilities along path\n",
    "    # dimension is len(obs) x len(start_p)\n",
    "    V = np.zeros((len(obs), len(start_p)))\n",
    "    V[0,:] = start_p+emit_p[:, obs[0]]\n",
    "    \n",
    "    # loop through sequence using V[t-1] \n",
    "    for t in range(1, len(obs)):\n",
    "        for y in range(len(start_p)):\n",
    "            V[t,y] = np.max(V[t-1,:] + trans_p[:, y] + emit_p[y, obs[t]])\n",
    "    # decoded sequence is max index of every row, return new indices mapped back to chars\n",
    "    return map(lambda x: intToChar[x], np.argmax(V, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "# first line correct\n",
    "# second line typo\n",
    "# third line viterbi decoding\n",
    "# output number of characters corrected\n",
    "# output number of new errors introduced\n",
    "def comparison(num_chars):\n",
    "    viterbi_res = viterbi(sh_typo_idx[0:100], start, transition, emission)\n",
    "    print ''.join(sh_correct[0:num_chars]) + \"(Correct)\"\n",
    "    print ''.join(sh_typo[0:num_chars]) + \"(Typos)\"\n",
    "    print ''.join(viterbi_res) + \"(Viterbi)\"\n",
    "    corrected = 0\n",
    "    new_err = 0\n",
    "    for i in range(num_chars):\n",
    "        if sh_correct[i] == sh_typo[i] and sh_correct[i] != viterbi_res[i]:\n",
    "            new_err += 1\n",
    "        if sh_correct[i] != sh_typo[i] and sh_correct[i] == viterbi_res[i]:\n",
    "            corrected += 1\n",
    "    print \"Corrected: \" + str(corrected)\n",
    "    print \"New Errors: \" + str(new_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase that thereby beautys rose might never die but as the riper(Correct)\n",
      "fron fbirest crebtuses we eesjsf iocrease uhat therfbz bebuuys rose night never eie buu as uie siper(Typos)\n",
      "fron fairest creatuses we eesise increase that thereay beattys rose night never eie but as the sioer(Viterbi)\n",
      "Corrected: 13\n",
      "New Errors: 3\n"
     ]
    }
   ],
   "source": [
    "comparison(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Problem 2: Final Project Review\n",
    "    \n",
    "You will be contacted shortly by a TF to meet and discuss your final project proposal. Be sure to take advantage of this feedback option. Review meetings should be scheduled within the week from April 11-15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
